{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是李宏毅[GAN课程](https://www.youtube.com/watch?v=DQNNMiAP5lw)的学习笔记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://i-blog.csdnimg.cn/blog_migrate/fcf7cf87abbc78b0e4bdd28de26c8dd4.png#pic_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:#find() 返回字符串第一次出现的索引，如果没有匹配项则返回-1\n",
    "        m.weight.data.normal_(0.0, 0.02)#归一化\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self,in_dims=1024,out_dim=512):\n",
    "        # in_dims指的是输出的随机向量的维度 ; out_dims指的是输出图像的尺寸\n",
    "        super(G,self).__init__()\n",
    "        self.model=nn.Sequential()\n",
    "\n",
    "        self.fn1=nn.Sequential(nn.Linear(in_dims,out_dim*1024),\n",
    "                               nn.BatchNorm1d(out_dim*1024),\n",
    "                               nn.ReLU()) #经过第一层线性层之后记得reshape为64*64*1024的形状\n",
    "        \n",
    "        self.arcconv1=nn.ConvTranspose2d(in_channels=2048,out_channels=1024,kernel_size=5,stride=4,padding=2,output_padding=3)\n",
    "        self.rl1=nn.ReLU()\n",
    "        self.arcconv2=nn.ConvTranspose2d(in_channels=1024,out_channels=256,kernel_size=3,stride=2,padding=1,output_padding=1)\n",
    "        self.rl2=nn.ReLU()\n",
    "        self.arcconv3=nn.ConvTranspose2d(in_channels=256,out_channels=64,kernel_size=3,stride=2,padding=1,output_padding=1)\n",
    "        self.rl3=nn.ReLU()\n",
    "        self.arcconv4=nn.ConvTranspose2d(in_channels=64,out_channels=3,kernel_size=3,stride=2,padding=1,output_padding=1)\n",
    "        self.rl4=nn.Tanh()\n",
    "\n",
    "        self.fn2=(self.arcconv1,self.rl1,self.arcconv2,self.rl2,self.arcconv3,self.rl3,self.arcconv4,self.rl4)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        z=self.f1(x)\n",
    "        z=z.view(-1,1024,64,64)\n",
    "        z=self.fn2(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(nn.Module):\n",
    "    def __init__(self,in_dims=3,dims=64):\n",
    "        # 输入的数据格式应该是  N*3*512*512\n",
    "        super(D, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "\n",
    "        def conv_binary_2d(in_dim,out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_dim,out_dim,5,2,2),\n",
    "                nn.BatchNorm2d(out_dim),nn.LeakyReLU(0.15)\n",
    "            )\n",
    "        self.model.append(nn.Conv2d(in_dims, dims, 10, 8, 1))\n",
    "        self.model.append(nn.LeakyReLU(0.1))\n",
    "        self.model.append(conv_binary_2d(dims,2*dims))\n",
    "        self.model.append(conv_binary_2d(2*dims,4*dims))\n",
    "        self.model.append(conv_binary_2d(4*dims,8*dims))\n",
    "        self.model.append(conv_binary_2d(8*dims,16*dims))\n",
    "        self.model.append(nn.Conv2d(16*dims,1,4))\n",
    "        self.model.append(nn.Sigmoid())\n",
    "        self.apply(weights_init)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # 返回是一个batch-size的一维数组\n",
    "        z=self.model(x)\n",
    "        z=z.view(-1)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(10, 10), stride=(8, 8), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.15)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.15)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.15)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.15)\n",
       "    )\n",
       "    (6): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练！\n",
    "import os\n",
    "\n",
    "batch_size=32\n",
    "feature_dim=128 #设置特征向量的大小\n",
    "\n",
    "lr=5e-5\n",
    "n_epoch=100\n",
    "\n",
    "workspace_dir = '.'\n",
    "save_dir=os.path.join(workspace_dir,'logs')\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "\n",
    "G_model = G(in_dims=feature_dim).cuda()\n",
    "D_model = D(3).cuda()\n",
    "G_model.train()\n",
    "D_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.BCELoss()\n",
    "\n",
    "# optimizer\n",
    "opt_D = torch.optim.Adam(D_model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_G = torch.optim.Adam(G_model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
