{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size must be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]  # Batch size\n",
    "        length = x.shape[1]  # Sequence length\n",
    "\n",
    "        # Split the embedding into multiple heads\n",
    "        values = self.values(x).view(N, length, self.heads, self.head_dim)\n",
    "        keys = self.keys(x).view(N, length, self.heads, self.head_dim)\n",
    "        queries = self.queries(x).view(N, length, self.heads, self.head_dim)\n",
    "\n",
    "        # Transpose to get dimensions for attention calculation\n",
    "        values = values.permute(0, 2, 1, 3)  # (N, heads, length, head_dim)\n",
    "        keys = keys.permute(0, 2, 1, 3)      # (N, heads, length, head_dim)\n",
    "        queries = queries.permute(0, 2, 1, 3)  # (N, heads, length, head_dim)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  # (N, heads, length, length)\n",
    "        attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, length, self.embed_size)\n",
    "        \n",
    "        # Pass through the final linear layer\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 256])\n",
      "tensor([[[ 0.0025,  0.0996, -0.0957,  ..., -0.0349,  0.1079, -0.0818],\n",
      "         [-0.0396,  0.1765, -0.1102,  ..., -0.0694,  0.1320, -0.0667],\n",
      "         [-0.0265,  0.1216, -0.0877,  ..., -0.0087,  0.0482, -0.0539],\n",
      "         ...,\n",
      "         [-0.0362,  0.1734, -0.1068,  ..., -0.0709,  0.1313, -0.0634],\n",
      "         [-0.0278,  0.1208, -0.0864,  ..., -0.0125,  0.0451, -0.0537],\n",
      "         [-0.0769,  0.0321, -0.1482,  ..., -0.0271,  0.1362, -0.0634]],\n",
      "\n",
      "        [[-0.0301,  0.1330, -0.0632,  ..., -0.0793,  0.1258, -0.0237],\n",
      "         [-0.0393,  0.1000, -0.0926,  ...,  0.0104,  0.1036, -0.0437],\n",
      "         [-0.0874,  0.0553, -0.0957,  ..., -0.0500,  0.1250, -0.1058],\n",
      "         ...,\n",
      "         [-0.0369,  0.0989, -0.0921,  ...,  0.0080,  0.1016, -0.0424],\n",
      "         [-0.0864,  0.0557, -0.0990,  ..., -0.0474,  0.1272, -0.1068],\n",
      "         [-0.0970,  0.1262, -0.1569,  ..., -0.0162,  0.0853, -0.0452]],\n",
      "\n",
      "        [[-0.0248,  0.0954, -0.0913,  ..., -0.0661,  0.1111,  0.0180],\n",
      "         [ 0.0015,  0.0580, -0.0946,  ..., -0.0561,  0.1437, -0.0726],\n",
      "         [-0.0565,  0.0538, -0.0672,  ..., -0.0293,  0.1261, -0.0738],\n",
      "         ...,\n",
      "         [ 0.0042,  0.0559, -0.0926,  ..., -0.0551,  0.1421, -0.0703],\n",
      "         [-0.0586,  0.0537, -0.0732,  ..., -0.0261,  0.1282, -0.0775],\n",
      "         [ 0.0189,  0.0503, -0.0864,  ..., -0.0428,  0.1008, -0.0518]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0320,  0.1388, -0.1242,  ..., -0.0065,  0.0972, -0.0252],\n",
      "         [-0.0513,  0.0888, -0.1024,  ..., -0.0032,  0.1022, -0.0534],\n",
      "         [-0.0458,  0.0940, -0.1014,  ..., -0.0161,  0.1345, -0.0745],\n",
      "         ...,\n",
      "         [-0.0500,  0.0872, -0.0996,  ..., -0.0029,  0.1022, -0.0552],\n",
      "         [-0.0485,  0.0922, -0.1014,  ..., -0.0189,  0.1316, -0.0771],\n",
      "         [-0.0703,  0.1620, -0.1112,  ..., -0.0202,  0.1140,  0.0242]],\n",
      "\n",
      "        [[ 0.0284,  0.1006, -0.1136,  ..., -0.0457,  0.0881, -0.0597],\n",
      "         [-0.0312,  0.0967, -0.1208,  ..., -0.0388,  0.1568, -0.0401],\n",
      "         [ 0.0152,  0.0942, -0.1468,  ...,  0.0171,  0.0998, -0.0400],\n",
      "         ...,\n",
      "         [-0.0345,  0.0956, -0.1220,  ..., -0.0393,  0.1589, -0.0447],\n",
      "         [ 0.0097,  0.0968, -0.1469,  ...,  0.0151,  0.0961, -0.0388],\n",
      "         [ 0.0208,  0.1266, -0.1447,  ..., -0.0446,  0.1155,  0.0148]],\n",
      "\n",
      "        [[ 0.0167,  0.0832, -0.1504,  ..., -0.0497,  0.0850,  0.0005],\n",
      "         [-0.0281,  0.1556, -0.1348,  ..., -0.1086,  0.0802, -0.0789],\n",
      "         [-0.0806,  0.1508, -0.1050,  ..., -0.0260,  0.1484, -0.0551],\n",
      "         ...,\n",
      "         [-0.0293,  0.1589, -0.1372,  ..., -0.1121,  0.0795, -0.0788],\n",
      "         [-0.0787,  0.1503, -0.1067,  ..., -0.0245,  0.1468, -0.0531],\n",
      "         [-0.0400,  0.1451, -0.1362,  ..., -0.0448,  0.0328,  0.0201]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "embed_size = 256  # Embedding size\n",
    "num_heads = 8     # Number of attention heads\n",
    "multihead_attention = MultiHeadAttention(embed_size, num_heads)\n",
    "\n",
    "# Sample input (batch_size, sequence_length, embedding_size)\n",
    "x = torch.rand(64, 32, embed_size)  # Example input\n",
    "output = multihead_attention(x)\n",
    "print(output.shape)  # Should be (64, 10, 256)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
