{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size must be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]  # Batch size\n",
    "        length = x.shape[1]  # Sequence length\n",
    "\n",
    "        # Split the embedding into multiple heads\n",
    "        values = self.values(x).view(N, length, self.heads, self.head_dim)\n",
    "        keys = self.keys(x).view(N, length, self.heads, self.head_dim)\n",
    "        queries = self.queries(x).view(N, length, self.heads, self.head_dim)\n",
    "\n",
    "        # Transpose to get dimensions for attention calculation\n",
    "        values = values.permute(0, 2, 1, 3)  # (N, heads, length, head_dim)\n",
    "        keys = keys.permute(0, 2, 1, 3)      # (N, heads, length, head_dim)\n",
    "        queries = queries.permute(0, 2, 1, 3)  # (N, heads, length, head_dim)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  # (N, heads, length, length)\n",
    "        attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, length, self.embed_size)\n",
    "        \n",
    "        # Pass through the final linear layer\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 256])\n",
      "tensor([[[ 0.0359, -0.0094,  0.1310,  ...,  0.0443, -0.0459,  0.0820],\n",
      "         [ 0.1151,  0.0488,  0.1066,  ..., -0.0399, -0.0244, -0.0112],\n",
      "         [ 0.0831, -0.0396,  0.0956,  ...,  0.0780, -0.0878,  0.0266],\n",
      "         ...,\n",
      "         [ 0.0779, -0.0408,  0.0997,  ...,  0.0731, -0.0889,  0.0288],\n",
      "         [ 0.0772,  0.0261,  0.0332,  ...,  0.0459, -0.0716,  0.0105],\n",
      "         [ 0.0982,  0.0036,  0.0868,  ...,  0.0141, -0.0214,  0.0481]],\n",
      "\n",
      "        [[ 0.0586, -0.0611,  0.0391,  ...,  0.0371, -0.0770,  0.0222],\n",
      "         [ 0.0758,  0.0019,  0.1511,  ...,  0.0600, -0.0333, -0.0160],\n",
      "         [ 0.0576, -0.0013,  0.1217,  ...,  0.0369, -0.0620,  0.0041],\n",
      "         ...,\n",
      "         [ 0.0622, -0.0016,  0.1240,  ...,  0.0355, -0.0647,  0.0038],\n",
      "         [ 0.0351, -0.0278,  0.1407,  ...,  0.0837, -0.0816,  0.0079],\n",
      "         [ 0.0376, -0.0127,  0.0706,  ...,  0.0588, -0.0509,  0.0212]],\n",
      "\n",
      "        [[-0.0069, -0.0388,  0.1328,  ...,  0.0380, -0.1013,  0.0230],\n",
      "         [ 0.0482, -0.0599,  0.1569,  ..., -0.0224, -0.0930,  0.0342],\n",
      "         [ 0.1065, -0.0625,  0.1147,  ..., -0.0234, -0.0440,  0.0542],\n",
      "         ...,\n",
      "         [ 0.1122, -0.0693,  0.1162,  ..., -0.0251, -0.0481,  0.0518],\n",
      "         [ 0.0349,  0.0090,  0.1045,  ..., -0.0013, -0.0478, -0.0438],\n",
      "         [ 0.0659,  0.0218,  0.1348,  ...,  0.0005, -0.0426,  0.0410]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0943,  0.0288,  0.0981,  ..., -0.0038, -0.0522,  0.0867],\n",
      "         [-0.0272, -0.0046,  0.1215,  ..., -0.0062, -0.0532,  0.0448],\n",
      "         [ 0.0638,  0.0058,  0.0884,  ...,  0.0030, -0.1139,  0.0527],\n",
      "         ...,\n",
      "         [ 0.0688,  0.0076,  0.0884,  ...,  0.0014, -0.1135,  0.0509],\n",
      "         [ 0.1303, -0.0252,  0.1373,  ...,  0.0321,  0.0133,  0.0834],\n",
      "         [ 0.0835, -0.0014,  0.0577,  ...,  0.0357, -0.0478,  0.0962]],\n",
      "\n",
      "        [[ 0.0060, -0.0353,  0.0807,  ..., -0.0109, -0.0786,  0.0492],\n",
      "         [ 0.0534,  0.0234,  0.0459,  ...,  0.0834, -0.1402,  0.0170],\n",
      "         [ 0.1134,  0.0105,  0.0833,  ...,  0.0035, -0.0986,  0.0838],\n",
      "         ...,\n",
      "         [ 0.1227,  0.0105,  0.0806,  ...,  0.0046, -0.1064,  0.0793],\n",
      "         [ 0.0110, -0.0288,  0.1238,  ...,  0.0544, -0.0579,  0.1121],\n",
      "         [ 0.1060, -0.0537,  0.0692,  ..., -0.0439, -0.1201,  0.0483]],\n",
      "\n",
      "        [[ 0.0839, -0.0367,  0.0270,  ..., -0.0033, -0.0542,  0.0317],\n",
      "         [ 0.0320, -0.0467,  0.1716,  ...,  0.0482, -0.1103,  0.0725],\n",
      "         [ 0.1092, -0.0008,  0.1094,  ...,  0.0484, -0.0382,  0.0247],\n",
      "         ...,\n",
      "         [ 0.1162, -0.0045,  0.1024,  ...,  0.0439, -0.0377,  0.0198],\n",
      "         [ 0.0649, -0.0282,  0.0755,  ...,  0.0362, -0.1160,  0.0083],\n",
      "         [ 0.0794,  0.0228,  0.0536,  ..., -0.0064, -0.0572, -0.0069]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "embed_size = 256  # Embedding size\n",
    "num_heads = 8     # Number of attention heads\n",
    "multihead_attention = MultiHeadAttention(embed_size, num_heads)\n",
    "\n",
    "# Sample input (batch_size, sequence_length, embedding_size)\n",
    "x = torch.rand(64, 32, embed_size)  # Example input\n",
    "output = multihead_attention(x)\n",
    "print(output.shape)  # Should be (64, 10, 256)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
